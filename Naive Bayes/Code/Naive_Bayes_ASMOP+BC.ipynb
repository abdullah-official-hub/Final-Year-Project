{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes_ASMOP+BC.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"10I2cpzb71FCKTkJejVzVS9tB5KmLwqM0","authorship_tag":"ABX9TyMK0e+OwI+vzQJpdHtlh/vw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NlRsIgfDDFE3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4b200f6a-9b74-4828-a80d-a20691a8181a","executionInfo":{"status":"ok","timestamp":1592058043718,"user_tz":-300,"elapsed":885,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["cd drive/\"My Drive\"/\"Fyp Data\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Fyp Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uT6wja81Ddlm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"c94c429e-4415-4860-a984-3ff28b5deba7","executionInfo":{"status":"ok","timestamp":1592058047500,"user_tz":-300,"elapsed":2802,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":[" asmOperators.csv                                     Knn_AsmOp_Model.ipynb\n"," AsmOpModel.joblib                                    label.csv\n"," ASM_OP_NB_Model.joblib                               SCModel.joblib\n"," byteCount.csv                                        SecCount_NB_Model.joblib\n"," byteCount.gsheet                                     sectionsCount.csv\n"," ByteCount_NB_Model.joblib                            sectionsCount.gsheet\n"," Code.py                                              sizes.csv\n","'Code to load data and create train test split.txt'   Sizes_NB_Model.joblib\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SNomt8zJDd-2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"84eab6ff-be77-4696-df28-eb13524e8f61","executionInfo":{"status":"ok","timestamp":1592058262858,"user_tz":-300,"elapsed":70397,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.externals import joblib \n","import nltk\n","from sklearn.externals import joblib \n","\n","\n","raw_data_ASM=[]\n","f1 = open('asmOperators.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_ASM.append(x)\n","f1.close()\n","\n","'''\n","raw_data_sizes=[]\n","f1 = open('sizes.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_sizes.append(x)\n","f1.close()\n","'''\n","\n","raw_data_bytes=[]\n","f1 = open('byteCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_bytes.append(x)\n","f1.close()\n","\n","'''\n","\n","raw_data_section=[]\n","f1 = open('sectionsCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_section.append(x)\n","f1.close()\n","'''\n","\n","label=[]\n","f1 = open('label.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  label.append(int(x[0])-1)\n","f1.close()\n","\n","\n","#raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)\n","#raw_data_section = tf.keras.utils.normalize(np.array(raw_data_section).astype(np.float32),axis=0)\n","raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)\n","#raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)\n","raw_data_ASM = tf.keras.utils.normalize(np.array(raw_data_ASM).astype(np.float32),axis=0)\n","\n","raw_data = raw_data_ASM\n","#raw_data = np.column_stack((raw_data,raw_data_section))\n","raw_data = np.column_stack((raw_data,raw_data_bytes))\n","#raw_data = np.column_stack((raw_data,raw_data_sizes))\n","#raw_data = np.column_stack((raw_data,raw_data_ASM))\n","#raw_data = np.column_stack((raw_data,raw_data))\n","\n","\n","print(np.array(raw_data).shape)\n","print(np.array(label).shape)\n","\n","raw_data = np.column_stack((raw_data,np.array(label)))\n","\n","flag=0\n","data_raw=[]\n","train_data=[]\n","test_data=[]\n","for x in raw_data:\n","  if int(x[len(x)-1]) != flag:\n","    totalElements = len(data_raw)\n","    percentElements =  int(totalElements * 0.7)\n","    train_data = train_data + data_raw[0:percentElements+1]\n","    test_data = test_data + data_raw[percentElements+1:totalElements]\n","    flag = flag + 1\n","    data_raw=[]\n","  else:\n","    data_raw.append(x)\n","\n","totalElements = len(data_raw)\n","percentElements =  int(totalElements * 0.7)\n","\n","train_data = train_data + data_raw[0:percentElements+1]\n","test_data = test_data + data_raw[percentElements+1:totalElements]\n","\n","\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","\n","train_data = np.array(train_data)\n","test_data = np.array(test_data)\n","\n","\"\"\"\n","print(train_data)\n","print(test_data)\n","\"\"\"\n","\n","train_label=[]\n","test_label=[]\n","clean_train=[]\n","clean_test=[]\n","print(\"New......: \")\n","trainDataRows, trainDataCols = np.array(train_data).shape\n","testDataRows, testDataCols = np.array(test_data).shape\n","print(trainDataRows, trainDataCols)\n","print(testDataRows, testDataCols)\n","\n","for x in range(0,len(train_data)):\n","  train_label.append(train_data[x][trainDataCols - 1])\n","  clean_train.append(train_data[x][0:trainDataCols - 1])\n","\n","\n","for x in range(0,len(test_data)):\n","  test_label.append(test_data[x][trainDataCols - 1])\n","  clean_test.append(test_data[x][0:trainDataCols - 1])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["(10868, 11400)\n","(10868,)\n","New......: \n","7605 11401\n","3255 11401\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P-cYsLDFECHy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"9eb84873-303a-4d6c-85dc-daee37615095","executionInfo":{"status":"ok","timestamp":1592058341455,"user_tz":-300,"elapsed":8298,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","model = GaussianNB()\n","model.fit(clean_train, train_label)\n","\n","train = model.predict(clean_train)\n","accuracyTrain = np.sum(train == train_label)/(np.array(train_label).shape)[0]\n","print(\"Training Accuracy: \", accuracyTrain)\n","\n","test = model.predict(clean_test)\n","accuracyTest = np.sum(test == test_label)/(np.array(test_label).shape)[0]\n","print(\"Testing Accuracy: \", accuracyTest)\n","joblib.dump(model, 'ASMOP+BC_NB_Model.joblib')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Training Accuracy:  0.8571992110453649\n","Testing Accuracy:  0.8058371735791091\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['ASMOP+BC_NB_Model.joblib']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ddB3mBNREken","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}