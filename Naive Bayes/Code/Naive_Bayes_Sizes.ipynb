{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes_Sizes.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1dN3J1MVbX1LJagm1y8_heLlSkqOATm0z","authorship_tag":"ABX9TyOpN0e5nEgEJF/VkAUd9S7y"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XMw10KkTqFUe","colab_type":"code","outputId":"fa26458d-d95b-4aaf-dd85-0b9850287cde","executionInfo":{"status":"ok","timestamp":1592051906621,"user_tz":-300,"elapsed":4564,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" asmOperators.csv                                     Knn_AsmOp_Model.ipynb\n"," AsmOpModel.joblib                                    label.csv\n"," ASM_OP_NB_Model.joblib                               SCModel.joblib\n"," byteCount.csv                                        sectionsCount.csv\n"," Code.py                                              sizes.csv\n","'Code to load data and create train test split.txt'   Sizes_NB_Model.joblib\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yD521eRXqMPE","colab_type":"code","outputId":"53d57dfc-94d2-4fe9-db51-f656e76f50c3","executionInfo":{"status":"ok","timestamp":1592052393372,"user_tz":-300,"elapsed":1549,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.externals import joblib \n","import nltk\n","from sklearn.externals import joblib \n","\n","'''\n","raw_data_ASM=[]\n","f1 = open('asmOperators.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_ASM.append(x)\n","f1.close()\n","'''\n","raw_data_sizes=[]\n","f1 = open('sizes.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_sizes.append(x)\n","f1.close()\n","\n","'''\n","\n","raw_data_bytes=[]\n","f1 = open('byteCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_bytes.append(x)\n","f1.close()\n","\n","\n","raw_data_section=[]\n","f1 = open('sectionsCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_section.append(x)\n","f1.close()\n","'''\n","\n","label=[]\n","f1 = open('label.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  label.append(int(x[0])-1)\n","f1.close()\n","\n","\n","#raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)\n","#raw_data_section = tf.keras.utils.normalize(np.array(raw_data_section).astype(np.float32),axis=0)\n","#raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)\n","raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)\n","#raw_data_ASM = tf.keras.utils.normalize(np.array(raw_data_ASM).astype(np.float32),axis=0)\n","\n","raw_data = raw_data_sizes\n","#raw_data = np.column_stack((raw_data,raw_data_section))\n","#raw_data = np.column_stack((raw_data,raw_data_bytes))\n","#raw_data = np.column_stack((raw_data,raw_data_sizes))\n","#raw_data = np.column_stack((raw_data,raw_data_ASM))\n","#raw_data = np.column_stack((raw_data,raw_data))\n","\n","\n","print(np.array(raw_data).shape)\n","print(np.array(label).shape)\n","\n","raw_data = np.column_stack((raw_data,np.array(label)))\n","\n","flag=0\n","data_raw=[]\n","train_data=[]\n","test_data=[]\n","for x in raw_data:\n","  if int(x[len(x)-1]) != flag:\n","    totalElements = len(data_raw)\n","    percentElements =  int(totalElements * 0.7)\n","    train_data = train_data + data_raw[0:percentElements+1]\n","    test_data = test_data + data_raw[percentElements+1:totalElements]\n","    flag = flag + 1\n","    data_raw=[]\n","  else:\n","    data_raw.append(x)\n","\n","totalElements = len(data_raw)\n","percentElements =  int(totalElements * 0.7)\n","\n","train_data = train_data + data_raw[0:percentElements+1]\n","test_data = test_data + data_raw[percentElements+1:totalElements]\n","\n","\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","\n","train_data = np.array(train_data)\n","test_data = np.array(test_data)\n","\n","\"\"\"\n","print(train_data)\n","print(test_data)\n","\"\"\"\n","\n","train_label=[]\n","test_label=[]\n","clean_train=[]\n","clean_test=[]\n","print(\"New......: \")\n","trainDataRows, trainDataCols = np.array(train_data).shape\n","testDataRows, testDataCols = np.array(test_data).shape\n","print(trainDataRows, trainDataCols)\n","print(testDataRows, testDataCols)\n","\n","for x in range(0,len(train_data)):\n","  train_label.append(train_data[x][trainDataCols - 1])\n","  clean_train.append(train_data[x][0:trainDataCols - 1])\n","\n","\n","for x in range(0,len(test_data)):\n","  test_label.append(test_data[x][trainDataCols - 1])\n","  clean_test.append(test_data[x][0:trainDataCols - 1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10868, 2)\n","(10868,)\n","New......: \n","7605 3\n","3255 3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-P4UGDC0qnWV","colab_type":"code","outputId":"ee166e9d-794e-41b3-f715-6e6d3e3c4873","executionInfo":{"status":"ok","timestamp":1592052404878,"user_tz":-300,"elapsed":809,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","model = GaussianNB()\n","model.fit(clean_train, train_label)\n","\n","train = model.predict(clean_train)\n","accuracyTrain = np.sum(train == train_label)/(np.array(train_label).shape)[0]\n","print(\"Training Accuracy: \", accuracyTrain)\n","\n","test = model.predict(clean_test)\n","accuracyTest = np.sum(test == test_label)/(np.array(test_label).shape)[0]\n","print(\"Testing Accuracy: \", accuracyTest)\n","joblib.dump(model, 'Sizes_NB_Model.joblib')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Accuracy:  0.5063773833004602\n","Testing Accuracy:  0.5035330261136712\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Sizes_NB_Model.joblib']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"MfKWVmgpqy2U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}