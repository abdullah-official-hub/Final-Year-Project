{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes_SecCount.ipynb","provenance":[],"mount_file_id":"18s9hWwmyDtEzp_8nmD8SkxqCNvZumnHJ","authorship_tag":"ABX9TyOo3iOh3yizkefV2F6uNWLK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"GhU-yrbwwNzC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b8d11ac5-7891-4c9f-c9a7-652397721f4e","executionInfo":{"status":"ok","timestamp":1592053064942,"user_tz":-300,"elapsed":1256,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["cd drive/\"My Drive\"/\"Fyp Data\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Fyp Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t4Myp621wV3J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"9cb34b8c-4ea6-4a4b-d4a4-f6a06e0787b5","executionInfo":{"status":"ok","timestamp":1592053320054,"user_tz":-300,"elapsed":7659,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":[" asmOperators.csv                                     Knn_AsmOp_Model.ipynb\n"," AsmOpModel.joblib                                    label.csv\n"," ASM_OP_NB_Model.joblib                               SCModel.joblib\n"," byteCount.csv                                        sectionsCount.csv\n"," byteCount.gsheet                                     sectionsCount.gsheet\n"," ByteCount_NB_Model.joblib                            sizes.csv\n"," Code.py                                              Sizes_NB_Model.joblib\n","'Code to load data and create train test split.txt'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0cc2_tjywfYu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"fe3a7ae5-ddaa-4ce1-fc79-7279e1577181","executionInfo":{"status":"ok","timestamp":1592053332236,"user_tz":-300,"elapsed":11170,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.externals import joblib \n","import nltk\n","from sklearn.externals import joblib \n","\n","'''\n","raw_data_ASM=[]\n","f1 = open('asmOperators.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_ASM.append(x)\n","f1.close()\n","\n","raw_data_sizes=[]\n","f1 = open('sizes.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_sizes.append(x)\n","f1.close()\n","\n","\n","raw_data_bytes=[]\n","f1 = open('byteCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_bytes.append(x)\n","f1.close()\n","\n","'''\n","\n","raw_data_section=[]\n","f1 = open('sectionsCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_section.append(x)\n","f1.close()\n","\n","label=[]\n","f1 = open('label.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  label.append(int(x[0])-1)\n","f1.close()\n","\n","\n","#raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)\n","raw_data_section = tf.keras.utils.normalize(np.array(raw_data_section).astype(np.float32),axis=0)\n","#raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)\n","#raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)\n","#raw_data_ASM = tf.keras.utils.normalize(np.array(raw_data_ASM).astype(np.float32),axis=0)\n","\n","raw_data = raw_data_section\n","#raw_data = np.column_stack((raw_data,raw_data_section))\n","#raw_data = np.column_stack((raw_data,raw_data_bytes))\n","#raw_data = np.column_stack((raw_data,raw_data_sizes))\n","#raw_data = np.column_stack((raw_data,raw_data_ASM))\n","#raw_data = np.column_stack((raw_data,raw_data))\n","\n","\n","print(np.array(raw_data).shape)\n","print(np.array(label).shape)\n","\n","raw_data = np.column_stack((raw_data,np.array(label)))\n","\n","flag=0\n","data_raw=[]\n","train_data=[]\n","test_data=[]\n","for x in raw_data:\n","  if int(x[len(x)-1]) != flag:\n","    totalElements = len(data_raw)\n","    percentElements =  int(totalElements * 0.7)\n","    train_data = train_data + data_raw[0:percentElements+1]\n","    test_data = test_data + data_raw[percentElements+1:totalElements]\n","    flag = flag + 1\n","    data_raw=[]\n","  else:\n","    data_raw.append(x)\n","\n","totalElements = len(data_raw)\n","percentElements =  int(totalElements * 0.7)\n","\n","train_data = train_data + data_raw[0:percentElements+1]\n","test_data = test_data + data_raw[percentElements+1:totalElements]\n","\n","\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","\n","train_data = np.array(train_data)\n","test_data = np.array(test_data)\n","\n","\"\"\"\n","print(train_data)\n","print(test_data)\n","\"\"\"\n","\n","train_label=[]\n","test_label=[]\n","clean_train=[]\n","clean_test=[]\n","print(\"New......: \")\n","trainDataRows, trainDataCols = np.array(train_data).shape\n","testDataRows, testDataCols = np.array(test_data).shape\n","print(trainDataRows, trainDataCols)\n","print(testDataRows, testDataCols)\n","\n","for x in range(0,len(train_data)):\n","  train_label.append(train_data[x][trainDataCols - 1])\n","  clean_train.append(train_data[x][0:trainDataCols - 1])\n","\n","\n","for x in range(0,len(test_data)):\n","  test_label.append(test_data[x][trainDataCols - 1])\n","  clean_test.append(test_data[x][0:trainDataCols - 1])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["(10868, 458)\n","(10868,)\n","New......: \n","7605 459\n","3255 459\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v5LSOX3KxDEC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"e587e3ad-680e-4631-eb69-367c31db6956","executionInfo":{"status":"ok","timestamp":1592053362579,"user_tz":-300,"elapsed":2380,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","model = GaussianNB()\n","model.fit(clean_train, train_label)\n","\n","train = model.predict(clean_train)\n","accuracyTrain = np.sum(train == train_label)/(np.array(train_label).shape)[0]\n","print(\"Training Accuracy: \", accuracyTrain)\n","\n","test = model.predict(clean_test)\n","accuracyTest = np.sum(test == test_label)/(np.array(test_label).shape)[0]\n","print(\"Testing Accuracy: \", accuracyTest)\n","joblib.dump(model, 'SecCount_NB_Model.joblib')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Training Accuracy:  0.5405654174884944\n","Testing Accuracy:  0.530568356374808\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['SecCount_NB_Model.joblib']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Z7Hs2zyjxmX0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}