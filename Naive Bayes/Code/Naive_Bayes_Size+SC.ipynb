{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes_Size+SC.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1wDO1SIuihgpCZWHe3gXXuvr-BXxPlYI9","authorship_tag":"ABX9TyM04GVvmFkK70R0oe7D3JuH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8UHrnWDxMUj9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fd5716e5-b421-4a2a-b44e-e04d74d008a2","executionInfo":{"status":"ok","timestamp":1592060406812,"user_tz":-300,"elapsed":1031,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["cd drive/\"My Drive\"/\"Fyp Data\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Fyp Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qkKW2c7jMefR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":165},"outputId":"dec5c617-428d-42bf-efbc-96dc37b49958","executionInfo":{"status":"ok","timestamp":1592060530252,"user_tz":-300,"elapsed":8496,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.externals import joblib \n","import nltk\n","from sklearn.externals import joblib \n","\n","'''\n","\n","raw_data_ASM=[]\n","f1 = open('asmOperators.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_ASM.append(x)\n","f1.close()\n","'''\n","\n","raw_data_sizes=[]\n","f1 = open('sizes.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_sizes.append(x)\n","f1.close()\n","'''\n","\n","raw_data_bytes=[]\n","f1 = open('byteCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_bytes.append(x)\n","f1.close()\n","'''\n","\n","raw_data_section=[]\n","f1 = open('sectionsCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_section.append(x)\n","f1.close()\n","\n","label=[]\n","f1 = open('label.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  label.append(int(x[0])-1)\n","f1.close()\n","\n","\n","#raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)\n","raw_data_section = tf.keras.utils.normalize(np.array(raw_data_section).astype(np.float32),axis=0)\n","#raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)\n","raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)\n","#raw_data_ASM = tf.keras.utils.normalize(np.array(raw_data_ASM).astype(np.float32),axis=0)\n","\n","raw_data = raw_data_section\n","#raw_data = np.column_stack((raw_data,raw_data_section))\n","#raw_data = np.column_stack((raw_data,raw_data_bytes))\n","raw_data = np.column_stack((raw_data,raw_data_sizes))\n","#raw_data = np.column_stack((raw_data,raw_data_ASM))\n","#raw_data = np.column_stack((raw_data,raw_data))\n","\n","\n","print(np.array(raw_data).shape)\n","print(np.array(label).shape)\n","\n","raw_data = np.column_stack((raw_data,np.array(label)))\n","\n","flag=0\n","data_raw=[]\n","train_data=[]\n","test_data=[]\n","for x in raw_data:\n","  if int(x[len(x)-1]) != flag:\n","    totalElements = len(data_raw)\n","    percentElements =  int(totalElements * 0.7)\n","    train_data = train_data + data_raw[0:percentElements+1]\n","    test_data = test_data + data_raw[percentElements+1:totalElements]\n","    flag = flag + 1\n","    data_raw=[]\n","  else:\n","    data_raw.append(x)\n","\n","totalElements = len(data_raw)\n","percentElements =  int(totalElements * 0.7)\n","\n","train_data = train_data + data_raw[0:percentElements+1]\n","test_data = test_data + data_raw[percentElements+1:totalElements]\n","\n","\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","\n","train_data = np.array(train_data)\n","test_data = np.array(test_data)\n","\n","\"\"\"\n","print(train_data)\n","print(test_data)\n","\"\"\"\n","\n","train_label=[]\n","test_label=[]\n","clean_train=[]\n","clean_test=[]\n","print(\"New......: \")\n","trainDataRows, trainDataCols = np.array(train_data).shape\n","testDataRows, testDataCols = np.array(test_data).shape\n","print(trainDataRows, trainDataCols)\n","print(testDataRows, testDataCols)\n","\n","for x in range(0,len(train_data)):\n","  train_label.append(train_data[x][trainDataCols - 1])\n","  clean_train.append(train_data[x][0:trainDataCols - 1])\n","\n","\n","for x in range(0,len(test_data)):\n","  test_label.append(test_data[x][trainDataCols - 1])\n","  clean_test.append(test_data[x][0:trainDataCols - 1])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["(10868, 460)\n","(10868,)\n","New......: \n","7605 461\n","3255 461\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oXFnPp5cM6y-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"4c9c543f-7941-42d8-8f76-18056c941fe3","executionInfo":{"status":"ok","timestamp":1592060554963,"user_tz":-300,"elapsed":995,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","model = GaussianNB()\n","model.fit(clean_train, train_label)\n","\n","train = model.predict(clean_train)\n","accuracyTrain = np.sum(train == train_label)/(np.array(train_label).shape)[0]\n","print(\"Training Accuracy: \", accuracyTrain)\n","\n","test = model.predict(clean_test)\n","accuracyTest = np.sum(test == test_label)/(np.array(test_label).shape)[0]\n","print(\"Testing Accuracy: \", accuracyTest)\n","joblib.dump(model, 'Size+SC_NB_Model.joblib')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training Accuracy:  0.5410913872452334\n","Testing Accuracy:  0.530568356374808\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Size+SC_NB_Model.joblib']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"F4o7YPt_NCqu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}