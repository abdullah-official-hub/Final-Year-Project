{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes_ByteCount.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1EDG1w3hj-eHvFTAr5aHE-Bge8NBHI9yW","authorship_tag":"ABX9TyNCHvxZvu7tRJnRHv0fd/nf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"81LH6NHyurHS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"48b58f53-4b65-4281-f799-15d7209b0321","executionInfo":{"status":"ok","timestamp":1592052645004,"user_tz":-300,"elapsed":1105,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["cd drive/\"My Drive\"/\"Fyp Data\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Fyp Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1iOX-hkcuvCm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"6a5caabd-8301-41df-dfe0-198081243e41","executionInfo":{"status":"ok","timestamp":1592052651714,"user_tz":-300,"elapsed":3140,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":[" asmOperators.csv                                     Knn_AsmOp_Model.ipynb\n"," AsmOpModel.joblib                                    label.csv\n"," ASM_OP_NB_Model.joblib                               SCModel.joblib\n"," byteCount.csv                                        sectionsCount.csv\n"," Code.py                                              sizes.csv\n","'Code to load data and create train test split.txt'   Sizes_NB_Model.joblib\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WNcNO4KWu4aV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"0176fddd-35d3-4e35-aa29-5e0d601f1a36","executionInfo":{"status":"ok","timestamp":1592052684770,"user_tz":-300,"elapsed":7377,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.externals import joblib \n","import nltk\n","from sklearn.externals import joblib \n","\n","'''\n","raw_data_ASM=[]\n","f1 = open('asmOperators.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_ASM.append(x)\n","f1.close()\n","\n","raw_data_sizes=[]\n","f1 = open('sizes.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_sizes.append(x)\n","f1.close()\n","\n","'''\n","raw_data_bytes=[]\n","f1 = open('byteCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_bytes.append(x)\n","f1.close()\n","\n","'''\n","\n","raw_data_section=[]\n","f1 = open('sectionsCount.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  raw_data_section.append(x)\n","f1.close()\n","'''\n","\n","label=[]\n","f1 = open('label.csv','rt')\n","data = csv.reader(f1)\n","for x in data:\n","  label.append(int(x[0])-1)\n","f1.close()\n","\n","\n","#raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)\n","#raw_data_section = tf.keras.utils.normalize(np.array(raw_data_section).astype(np.float32),axis=0)\n","raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)\n","#raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)\n","#raw_data_ASM = tf.keras.utils.normalize(np.array(raw_data_ASM).astype(np.float32),axis=0)\n","\n","raw_data = raw_data_bytes\n","#raw_data = np.column_stack((raw_data,raw_data_section))\n","#raw_data = np.column_stack((raw_data,raw_data_bytes))\n","#raw_data = np.column_stack((raw_data,raw_data_sizes))\n","#raw_data = np.column_stack((raw_data,raw_data_ASM))\n","#raw_data = np.column_stack((raw_data,raw_data))\n","\n","\n","print(np.array(raw_data).shape)\n","print(np.array(label).shape)\n","\n","raw_data = np.column_stack((raw_data,np.array(label)))\n","\n","flag=0\n","data_raw=[]\n","train_data=[]\n","test_data=[]\n","for x in raw_data:\n","  if int(x[len(x)-1]) != flag:\n","    totalElements = len(data_raw)\n","    percentElements =  int(totalElements * 0.7)\n","    train_data = train_data + data_raw[0:percentElements+1]\n","    test_data = test_data + data_raw[percentElements+1:totalElements]\n","    flag = flag + 1\n","    data_raw=[]\n","  else:\n","    data_raw.append(x)\n","\n","totalElements = len(data_raw)\n","percentElements =  int(totalElements * 0.7)\n","\n","train_data = train_data + data_raw[0:percentElements+1]\n","test_data = test_data + data_raw[percentElements+1:totalElements]\n","\n","\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","np.random.shuffle(train_data)\n","\n","train_data = np.array(train_data)\n","test_data = np.array(test_data)\n","\n","\"\"\"\n","print(train_data)\n","print(test_data)\n","\"\"\"\n","\n","train_label=[]\n","test_label=[]\n","clean_train=[]\n","clean_test=[]\n","print(\"New......: \")\n","trainDataRows, trainDataCols = np.array(train_data).shape\n","testDataRows, testDataCols = np.array(test_data).shape\n","print(trainDataRows, trainDataCols)\n","print(testDataRows, testDataCols)\n","\n","for x in range(0,len(train_data)):\n","  train_label.append(train_data[x][trainDataCols - 1])\n","  clean_train.append(train_data[x][0:trainDataCols - 1])\n","\n","\n","for x in range(0,len(test_data)):\n","  test_label.append(test_data[x][trainDataCols - 1])\n","  clean_test.append(test_data[x][0:trainDataCols - 1])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["(10868, 257)\n","(10868,)\n","New......: \n","7605 258\n","3255 258\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"loD0RA8Ju_q0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ec72f141-6a5c-48c7-da6b-639911e32a6e","executionInfo":{"status":"ok","timestamp":1592052904267,"user_tz":-300,"elapsed":912,"user":{"displayName":"ibn.e. Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjESA3ZGY6AJiCEOxdF5-mUD9aFuhtJ1qB-_cI2=s64","userId":"09097105324377109676"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","model = GaussianNB()\n","model.fit(clean_train, train_label)\n","\n","train = model.predict(clean_train)\n","accuracyTrain = np.sum(train == train_label)/(np.array(train_label).shape)[0]\n","print(\"Training Accuracy: \", accuracyTrain)\n","\n","test = model.predict(clean_test)\n","accuracyTest = np.sum(test == test_label)/(np.array(test_label).shape)[0]\n","print(\"Testing Accuracy: \", accuracyTest)\n","joblib.dump(model, 'ByteCount_NB_Model.joblib')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Training Accuracy:  0.6740302432610125\n","Testing Accuracy:  0.6611367127496159\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['ByteCount_NB_Model.joblib']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"VpoqZ74Wv21h","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}