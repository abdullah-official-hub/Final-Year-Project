# -*- coding: utf-8 -*-
"""Neural Network with Accuracy 98.77

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xEfir99lT8b-8tCMZ6f88Qq6qA80VOS2
"""

import matplotlib.pyplot as plt
import csv
import numpy as np
import tensorflow as tf

raw_data=[]
f1 = open('sectionsCount.csv','rt')
#next(f1)
data = csv.reader(f1)
for x in data:
  raw_data.append(x)
f1.close()

raw_data_PCA=[]
'''with open("asmOperators.csv","r",encoding="ISO-8859-1") as f:
  next(f)
  for line in f:
    line = (line.rstrip()).split(",")
    raw_data_PCA.append(line[1:])'''
#f1 = open('asmPCA.csv','rt')
f1=open('asmOperators.csv','rt')
data = csv.reader(f1)
for x in data:
  raw_data_PCA.append(x)
f1.close()

raw_data_sizes=[]
f1 = open('sizes.csv','rt')
data = csv.reader(f1)
for x in data:
  raw_data_sizes.append(x)
f1.close()

raw_data_bytes=[]
f1 = open('byteCount.csv','rt')
data = csv.reader(f1)
for x in data:
  raw_data_bytes.append(x)
f1.close()

label=[]
f1 = open('label.csv','rt')
data = csv.reader(f1)
for x in data:
  label.append(int(x[0])-1)
f1.close()


raw_data = tf.keras.utils.normalize(np.array(raw_data).astype(np.float32),axis=0)
raw_data_bytes = tf.keras.utils.normalize(np.array(raw_data_bytes).astype(np.float32),axis=0)
raw_data_sizes = tf.keras.utils.normalize(np.array(raw_data_sizes).astype(np.float32),axis=0)
raw_data_PCA = tf.keras.utils.normalize(np.array(raw_data_PCA).astype(np.float32),axis=0)

#print(np.array(raw_data).shape)
#print(np.array(raw_data_bytes).shape)

raw_data = np.column_stack((raw_data,raw_data_bytes))
raw_data = np.column_stack((raw_data,raw_data_sizes))
raw_data = np.column_stack((raw_data,raw_data_PCA))

print(np.array(raw_data).shape)
print(np.array(label).shape)

raw_data = np.column_stack((raw_data,np.array(label)))

flag=0
data_raw=[]
train_data=[]
test_data=[]
for x in raw_data:
  if int(x[len(x)-1]) != flag:
    totalElements = len(data_raw)
    percentElements =  int(totalElements * 0.7)
    train_data = train_data + data_raw[0:percentElements+1]
    test_data = test_data + data_raw[percentElements+1:totalElements]
    flag = flag + 1
    data_raw=[]
  else:
    data_raw.append(x)

totalElements = len(data_raw)
percentElements =  int(totalElements * 0.7)

train_data = train_data + data_raw[0:percentElements+1]
test_data = test_data + data_raw[percentElements+1:totalElements]


np.random.shuffle(train_data)
np.random.shuffle(train_data)
np.random.shuffle(train_data)

train_data = np.array(train_data)
test_data = np.array(test_data)

"""print(train_data)
print(test_data)"""

train_label=[]
test_label=[]
clean_train=[]
clean_test=[]

print(np.array(train_data).shape)
print(np.array(test_data).shape)

for x in range(0,len(train_data)):
  train_label.append(train_data[x][11861])
  clean_train.append(train_data[x][0:11861])


for x in range(0,len(test_data)):
  test_label.append(test_data[x][11861])
  clean_test.append(test_data[x][0:11861])


print(np.array(train_label))
print(np.array(clean_train))
print(np.array(test_label))
print(np.array(clean_test))


model  = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(9, activation=tf.nn.softmax))

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

clean_train = tf.keras.utils.normalize(np.array(clean_train).astype(np.float32), axis=1)

model.fit(np.array(clean_train).astype(np.float32),np.array(train_label).T,epochs=20,verbose=1)
model.summary()
max_val_accu=0
for x in range(1,1000):
  print(x)
  model.fit(np.array(clean_train).astype(np.float32),np.array(train_label).T,epochs=1,verbose=0)
  val_loss, val_acc = model.evaluate(np.array(clean_test),np.array(test_label).T,verbose=0)
  if int(val_acc * 100000) > max_val_accu:
    max_val_accu = val_acc * 100000
    model.save('All_Neural_Network.model')
    print(max_val_accu)

model.fit(np.array(clean_train).astype(np.float32),np.array(train_label).T,epochs=1)

val_loss, val_acc = model.evaluate(np.array(clean_test),np.array(test_label).T)
print(val_loss, val_acc)
print(max_val_accu)
'''
get_ipython().system_raw("unrar x data.rar") #to Unzip the data.

cd sample_data

get_ipython().system_raw("unrar x asmOperators.rar")

model.fit(np.array(clean_train).astype(np.float32),np.array(train_label).T,epochs=20,verbose=1)

val_loss, val_acc = model.evaluate(np.array(clean_test),np.array(test_label).T)

'''